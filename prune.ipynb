{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "WANDB_PROJECT = \"shearllama\"\n",
    "ENTITY = \"capecape\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_id: str = \"mistralai/Mistral-7B-v0.1\"\n",
    "    output_name: str = \"models/mistral_7b_12_layers_start\"\n",
    "    layers_ids: list = field(default_factory=lambda: [0,1,2,3,4,5,6,7])\n",
    "    save_tokenizer: bool = True\n",
    "    device_map: str = \"cuda:0\"\n",
    "    random: bool = False\n",
    "    log: bool = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config  = Config()\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(config.model_id)\n",
    "model_config.num_hidden_layers = len(config.layers_ids)\n",
    "logging.info(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=config.device_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = AutoModelForCausalLM.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_state_dict(original_model, layer_ids=[0,1,30,31], layer_naming=\"layers\"):\n",
    "    \"We will map the parameters of the original model layer_ids to the new model layer_ids\"\n",
    "    name_mapping = {}\n",
    "    layer_mapping = {layer_id: i for i, layer_id in enumerate(layer_ids)}\n",
    "    print(f\"Layer mapping: {layer_mapping}\")\n",
    "    for name, _ in original_model.named_parameters():\n",
    "        if layer_naming in name:\n",
    "            layer_id = int(name.split(\".\")[2])\n",
    "            if layer_id in layer_ids:\n",
    "                new_name = name.replace(f\"{layer_naming}.{layer_id}\", f\"{layer_naming}.{layer_mapping[layer_id]}\")\n",
    "                name_mapping[name] = new_name\n",
    "        else:\n",
    "            name_mapping[name] = name\n",
    "    return name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer mapping: {0: 0, 1: 1, 30: 2, 31: 3}\n"
     ]
    }
   ],
   "source": [
    "name_mapping = map_state_dict(original_model, [0,1,30,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually copy weights and biases\n",
    "for old_name, new_name in name_mapping.items():\n",
    "    # Check if the mapped name exists in the new model's state_dict\n",
    "    if new_name in new_model.state_dict():\n",
    "        # Directly load the parameter from the old model to the new model based on the mapping\n",
    "        new_model.state_dict()[new_name].data.copy_(original_model.state_dict()[old_name].data)\n",
    "    else:\n",
    "        print(f\"{new_name} not found in the new model's state_dict. Check your mapping dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007044096"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.to(torch.bfloat16)\n",
    "new_model.save_pretrained(\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
